Output folder created: results_2/step3-20251114_025151/step4-20251117_033952
{'accuracy': {'min': 0, 'max': 1}, 'bert_scores': {'min': -0.4734372794628143, 'max': 0.445616602897644}, 'bart_scores': {'min': -7.251434326171875, 'max': -2.1870484352111816}, 'alignscore': {'min': 0.0014057521475479, 'max': 0.7177547812461853}, 'rougeL': {'min': 0.0, 'max': 0.4126984126984126}, 'meteor': {'min': 0.0179856115107913, 'max': 0.458393516786538}}
{'accuracy': {'min': 0, 'max': 1}, 'bert_scores': {'min': -0.4382602274417877, 'max': 0.445616602897644}, 'bart_scores': {'min': -5.631471633911133, 'max': -2.1870484352111816}, 'alignscore': {'min': 0.001722790999338, 'max': 0.7177547812461853}, 'rougeL': {'min': 0.0263157894736842, 'max': 0.4126984126984126}, 'meteor': {'min': 0.0357142857142857, 'max': 0.458393516786538}}
39
50
Saved normalised data: results_2/step3-20251114_025151/step4-20251117_033952/med42-8b_norm.csv
Saved normalised data without errors: results_2/step3-20251114_025151/step4-20251117_033952/med42-8b_norm_no_error.csv
Saved normalised data: results_2/step3-20251114_025151/step4-20251117_033952/llama3-8b_norm.csv
Saved normalised data without errors: results_2/step3-20251114_025151/step4-20251117_033952/llama3-8b_norm_no_error.csv
Saved normalised data: results_2/step3-20251114_025151/step4-20251117_033952/med42-70b_norm.csv
Saved normalised data without errors: results_2/step3-20251114_025151/step4-20251117_033952/med42-70b_norm_no_error.csv
Saved normalised data: results_2/step3-20251114_025151/step4-20251117_033952/llama3.3-70b_norm.csv
Saved normalised data without errors: results_2/step3-20251114_025151/step4-20251117_033952/llama3.3-70b_norm_no_error.csv
Saved normalised data: results_2/step3-20251114_025151/step4-20251117_033952/deepseek-r1-1.5_norm.csv
Saved normalised data without errors: results_2/step3-20251114_025151/step4-20251117_033952/deepseek-r1-1.5_norm_no_error.csv

=== Calculating metrics for all models ===
Completed calculations for med42-8b
Completed calculations for llama3-8b
Completed calculations for med42-70b
Completed calculations for llama3.3-70b
Completed calculations for deepseek-r1-1.5
[{'accuracy': {'ci_lower': np.float64(0.8139169889916906),
               'ci_upper': np.float64(0.5460830110083095),
               'mean': np.float64(0.68),
               'n': 50,
               'standard_error': np.float64(0.06663945022680343)},
  'metric_means': {'alignscore': {'ci_lower': np.float64(0.24227902045863242),
                                  'ci_upper': np.float64(0.15121996553654105),
                                  'mean': np.float64(0.19674949299758673),
                                  'n': 50,
                                  'standard_error': np.float64(0.022656294036587796)},
                   'bart_scores': {'ci_lower': np.float64(0.5665437266152435),
                                   'ci_upper': np.float64(0.48075355216136817),
                                   'mean': np.float64(0.5236486393883059),
                                   'n': 50,
                                   'standard_error': np.float64(0.021345350218494475)},
                   'bert_scores': {'ci_lower': np.float64(0.5686368720199917),
                                   'ci_upper': np.float64(0.4907191188497522),
                                   'mean': np.float64(0.5296779954348719),
                                   'n': 50,
                                   'standard_error': np.float64(0.019386622538588832)},
                   'meteor': {'ci_lower': np.float64(0.4333632718872937),
                              'ci_upper': np.float64(0.35788915657825854),
                              'mean': np.float64(0.3956262142327761),
                              'n': 50,
                              'standard_error': np.float64(0.018778623938671992)},
                   'rougeL': {'ci_lower': np.float64(0.4055591288607419),
                              'ci_upper': np.float64(0.33667439556733864),
                              'mean': np.float64(0.3711167622140403),
                              'n': 50,
                              'standard_error': np.float64(0.017139127717310028)}},
  'model_name': 'med42-8b',
  'weighted_normalized_score': {'ci_lower': np.float64(0.43021935643214243),
                                'ci_upper': np.float64(0.37650828527488994),
                                'mean': np.float64(0.4033638208535162),
                                'n': 50,
                                'standard_error': np.float64(0.01336378707422296)}},
 {'accuracy': {'ci_lower': np.float64(0.740640969448547),
               'ci_upper': np.float64(0.459359030551453),
               'mean': np.float64(0.6),
               'n': 50,
               'standard_error': np.float64(0.06998542122237653)},
  'metric_means': {'alignscore': {'ci_lower': np.float64(0.29924167803295787),
                                  'ci_upper': np.float64(0.18233279252208262),
                                  'mean': np.float64(0.24078723527752025),
                                  'n': 50,
                                  'standard_error': np.float64(0.02908795932365399)},
                   'bart_scores': {'ci_lower': np.float64(0.615559777747905),
                                   'ci_upper': np.float64(0.5324128985760658),
                                   'mean': np.float64(0.5739863381619854),
                                   'n': 50,
                                   'standard_error': np.float64(0.020687675095613242)},
                   'bert_scores': {'ci_lower': np.float64(0.5645695959759297),
                                   'ci_upper': np.float64(0.4862180354473615),
                                   'mean': np.float64(0.5253938157116456),
                                   'n': 50,
                                   'standard_error': np.float64(0.019494557626142085)},
                   'meteor': {'ci_lower': np.float64(0.44034076317161025),
                              'ci_upper': np.float64(0.35694294748898253),
                              'mean': np.float64(0.3986418553302964),
                              'n': 50,
                              'standard_error': np.float64(0.020750110307331645)},
                   'rougeL': {'ci_lower': np.float64(0.4024156747466723),
                              'ci_upper': np.float64(0.3250299830404645),
                              'mean': np.float64(0.3637228288935684),
                              'n': 50,
                              'standard_error': np.float64(0.01925424096505998)}},
  'model_name': 'llama3-8b',
  'weighted_normalized_score': {'ci_lower': np.float64(0.45290519408022417),
                                'ci_upper': np.float64(0.38810763526978226),
                                'mean': np.float64(0.4205064146750032),
                                'n': 50,
                                'standard_error': np.float64(0.01612220274544384)}},
 {'accuracy': {'ci_lower': np.float64(0.8267018190027571),
               'ci_upper': np.float64(0.523298180997243),
               'mean': np.float64(0.675),
               'n': 40,
               'standard_error': np.float64(0.07499999999999998)},
  'metric_means': {'alignscore': {'ci_lower': np.float64(0.2407387446758451),
                                  'ci_upper': np.float64(0.1273434366070859),
                                  'mean': np.float64(0.1840410906414655),
                                  'n': 40,
                                  'standard_error': np.float64(0.028030804643820306)},
                   'bart_scores': {'ci_lower': np.float64(0.45439629696682016),
                                   'ci_upper': np.float64(0.3440048474125017),
                                   'mean': np.float64(0.39920057218966093),
                                   'n': 40,
                                   'standard_error': np.float64(0.027288264475007436)},
                   'bert_scores': {'ci_lower': np.float64(0.5554648039297457),
                                   'ci_upper': np.float64(0.4624254887003417),
                                   'mean': np.float64(0.5089451463150437),
                                   'n': 40,
                                   'standard_error': np.float64(0.02299889575509469)},
                   'meteor': {'ci_lower': np.float64(0.48313040796157225),
                              'ci_upper': np.float64(0.38445829219302097),
                              'mean': np.float64(0.4337943500772966),
                              'n': 40,
                              'standard_error': np.float64(0.024391298440880435)},
                   'rougeL': {'ci_lower': np.float64(0.3514320231428621),
                              'ci_upper': np.float64(0.2691745743081136),
                              'mean': np.float64(0.31030329872548784),
                              'n': 40,
                              'standard_error': np.float64(0.02033366739819387)}},
  'model_name': 'med42-70b',
  'weighted_normalized_score': {'ci_lower': np.float64(0.4023891831173602),
                                'ci_upper': np.float64(0.3321246000622217),
                                'mean': np.float64(0.36725689158979097),
                                'n': 40,
                                'standard_error': np.float64(0.017369085498703266)}},
 {'accuracy': {'ci_lower': np.float64(0.9148328706930995),
               'ci_upper': np.float64(0.6851671293069006),
               'mean': np.float64(0.8),
               'n': 50,
               'standard_error': np.float64(0.05714285714285715)},
  'metric_means': {'alignscore': {'ci_lower': np.float64(0.43760830257309735),
                                  'ci_upper': np.float64(0.2713702244261138),
                                  'mean': np.float64(0.3544892634996056),
                                  'n': 50,
                                  'standard_error': np.float64(0.04136149646839337)},
                   'bart_scores': {'ci_lower': np.float64(0.6931904718120194),
                                   'ci_upper': np.float64(0.5784789052291541),
                                   'mean': np.float64(0.6358346885205868),
                                   'n': 50,
                                   'standard_error': np.float64(0.028541247041522912)},
                   'bert_scores': {'ci_lower': np.float64(0.6136714711180239),
                                   'ci_upper': np.float64(0.517768464872233),
                                   'mean': np.float64(0.5657199679951285),
                                   'n': 50,
                                   'standard_error': np.float64(0.023861511744838238)},
                   'meteor': {'ci_lower': np.float64(0.3270761154026135),
                              'ci_upper': np.float64(0.23913828568692527),
                              'mean': np.float64(0.2831072005447694),
                              'n': 50,
                              'standard_error': np.float64(0.021879705743515988)},
                   'rougeL': {'ci_lower': np.float64(0.45329639602772626),
                              'ci_upper': np.float64(0.3428503969327857),
                              'mean': np.float64(0.39807339648025597),
                              'n': 50,
                              'standard_error': np.float64(0.027479936320452775)}},
  'model_name': 'llama3.3-70b',
  'weighted_normalized_score': {'ci_lower': np.float64(0.48847424821560564),
                                'ci_upper': np.float64(0.40641555860053274),
                                'mean': np.float64(0.4474449034080692),
                                'n': 50,
                                'standard_error': np.float64(0.020416923959586872)}},
 {'accuracy': {'ci_lower': np.float64(0.5717899018666606),
               'ci_upper': np.float64(0.24872291864615986),
               'mean': np.float64(0.41025641025641024),
               'n': 39,
               'standard_error': np.float64(0.07979349797082042)},
  'metric_means': {'alignscore': {'ci_lower': np.float64(0.1756079156475606),
                                  'ci_upper': np.float64(0.08512028080616121),
                                  'mean': np.float64(0.1303640982268609),
                                  'n': 39,
                                  'standard_error': np.float64(0.022349312316367217)},
                   'bart_scores': {'ci_lower': np.float64(0.41533356718540176),
                                   'ci_upper': np.float64(0.305198081175344),
                                   'mean': np.float64(0.3602658241803729),
                                   'n': 39,
                                   'standard_error': np.float64(0.027202085437064875)},
                   'bert_scores': {'ci_lower': np.float64(0.5053473531355634),
                                   'ci_upper': np.float64(0.41318748209969003),
                                   'mean': np.float64(0.45926741761762674),
                                   'n': 39,
                                   'standard_error': np.float64(0.02276233370920768)},
                   'meteor': {'ci_lower': np.float64(0.2704204544033162),
                              'ci_upper': np.float64(0.16114373423595962),
                              'mean': np.float64(0.2157820943196379),
                              'n': 39,
                              'standard_error': np.float64(0.026989981031211002)},
                   'rougeL': {'ci_lower': np.float64(0.30754106182992486),
                              'ci_upper': np.float64(0.22439495122212566),
                              'mean': np.float64(0.26596800652602526),
                              'n': 39,
                              'standard_error': np.float64(0.020536047793954914)}},
  'model_name': 'deepseek-r1-1.5',
  'weighted_normalized_score': {'ci_lower': np.float64(0.31765305333176547),
                                'ci_upper': np.float64(0.2550059230164439),
                                'mean': np.float64(0.2863294881741047),
                                'n': 39,
                                'standard_error': np.float64(0.015473056441306206)}}]

================================================================================
INDIVIDUAL MODEL COMPARISONS - WILCOXON RANK-SUM TESTS
================================================================================

--- Metric: accuracy ---

=== Wilcoxon rank-sum tests for metric 'accuracy' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1350.0000, p_raw = 0.4094, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1005.0000, p_raw = 0.964, p_bonf = 1 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 1100.0000, p_raw = 0.1749, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1238.0000, p_raw = 0.01155, p_bonf = 0.1155 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 925.0000, p_raw = 0.4686, p_bonf = 1 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1000.0000, p_raw = 0.03024, p_bonf = 0.3024 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1160.0000, p_raw = 0.07798, p_bonf = 0.7798 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 875.0000, p_raw = 0.1809, p_bonf = 1 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 986.5000, p_raw = 0.01921, p_bonf = 0.1921 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1355.0000, p_raw = 0.0001763, p_bonf = 0.001763 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.4093656768707191),
  'u_stat': np.float64(1350.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.963995963427891),
  'u_stat': np.float64(1005.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.1749436616380139),
  'u_stat': np.float64(1100.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.1154981152811939),
  'p_raw': np.float64(0.01154981152811939),
  'u_stat': np.float64(1238.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.4686221544107133),
  'u_stat': np.float64(925.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.3024262394085141),
  'p_raw': np.float64(0.030242623940851407),
  'u_stat': np.float64(1000.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.7797928872026194),
  'p_raw': np.float64(0.07797928872026194),
  'u_stat': np.float64(1160.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.18087442308410695),
  'u_stat': np.float64(875.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': np.float64(0.19205710844244667),
  'p_raw': np.float64(0.019205710844244668),
  'u_stat': np.float64(986.5)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.0017632643280804322),
  'p_raw': np.float64(0.00017632643280804324),
  'u_stat': np.float64(1355.0)}]

--- Metric: bert_scores ---

=== Wilcoxon rank-sum tests for metric 'bert_scores' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1299.0000, p_raw = 0.7381, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1113.0000, p_raw = 0.361, p_bonf = 1 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 1091.0000, p_raw = 0.2745, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1260.0000, p_raw = 0.01865, p_bonf = 0.1865 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 1079.0000, p_raw = 0.5239, p_bonf = 1 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1066.0000, p_raw = 0.2059, p_bonf = 1 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1227.0000, p_raw = 0.03756, p_bonf = 0.3756 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 751.0000, p_raw = 0.04361, p_bonf = 0.4361 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 937.0000, p_raw = 0.1249, p_bonf = 1 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1356.0000, p_raw = 0.001653, p_bonf = 0.01653 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.7381152654468986),
  'u_stat': np.float64(1299.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.3609808501545294),
  'u_stat': np.float64(1113.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.2745385184010334),
  'u_stat': np.float64(1091.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.18646412918753272),
  'p_raw': np.float64(0.018646412918753272),
  'u_stat': np.float64(1260.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.523852355911524),
  'u_stat': np.float64(1079.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.20586561131641923),
  'u_stat': np.float64(1066.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.37557734515413255),
  'p_raw': np.float64(0.037557734515413256),
  'u_stat': np.float64(1227.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(0.4361054373712675),
  'p_raw': np.float64(0.04361054373712675),
  'u_stat': np.float64(751.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.12487996021633678),
  'u_stat': np.float64(937.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.016532330016988468),
  'p_raw': np.float64(0.0016532330016988468),
  'u_stat': np.float64(1356.0)}]

--- Metric: bart_scores ---

=== Wilcoxon rank-sum tests for metric 'bart_scores' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1045.0000, p_raw = 0.1586, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1422.0000, p_raw = 0.0006203, p_bonf = 0.006203 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 883.0000, p_raw = 0.01152, p_bonf = 0.1152 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1490.0000, p_raw = 2.096e-05, p_bonf = 0.0002096 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 1543.0000, p_raw = 1.057e-05, p_bonf = 0.0001057 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1041.0000, p_raw = 0.1506, p_bonf = 1 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1601.0000, p_raw = 2.313e-07, p_bonf = 2.313e-06 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 389.0000, p_raw = 7.15e-07, p_bonf = 7.15e-06 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 882.0000, p_raw = 0.3196, p_bonf = 1 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1651.0000, p_raw = 2.328e-08, p_bonf = 2.328e-07 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.15860283312349666),
  'u_stat': np.float64(1045.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.006203181983505718),
  'p_raw': np.float64(0.0006203181983505718),
  'u_stat': np.float64(1422.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.1151775183712058),
  'p_raw': np.float64(0.01151775183712058),
  'u_stat': np.float64(883.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.00020961836712104262),
  'p_raw': np.float64(2.0961836712104263e-05),
  'u_stat': np.float64(1490.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.00010574053322830507),
  'p_raw': np.float64(1.0574053322830506e-05),
  'u_stat': np.float64(1543.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.15061531823974886),
  'u_stat': np.float64(1041.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(2.3129378297452723e-06),
  'p_raw': np.float64(2.3129378297452722e-07),
  'u_stat': np.float64(1601.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(7.149889364094457e-06),
  'p_raw': np.float64(7.149889364094457e-07),
  'u_stat': np.float64(389.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.319595538594893),
  'u_stat': np.float64(882.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(2.327640270979875e-07),
  'p_raw': np.float64(2.327640270979875e-08),
  'u_stat': np.float64(1651.0)}]

--- Metric: alignscore ---

=== Wilcoxon rank-sum tests for metric 'alignscore' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1125.0000, p_raw = 0.3907, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1101.0000, p_raw = 0.4145, p_bonf = 1 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 888.0000, p_raw = 0.0127, p_bonf = 0.127 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1253.0000, p_raw = 0.02175, p_bonf = 0.2175 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 1169.0000, p_raw = 0.1712, p_bonf = 1 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 993.0000, p_raw = 0.07702, p_bonf = 0.7702 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1327.0000, p_raw = 0.003654, p_bonf = 0.03654 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 662.0000, p_raw = 0.006135, p_bonf = 0.06135 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 962.0000, p_raw = 0.07512, p_bonf = 0.7512 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1450.0000, p_raw = 8.722e-05, p_bonf = 0.0008722 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.390737553235738),
  'u_stat': np.float64(1125.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.41446700243206114),
  'u_stat': np.float64(1101.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.12698459870504322),
  'p_raw': np.float64(0.012698459870504322),
  'u_stat': np.float64(888.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.217536153863713),
  'p_raw': np.float64(0.0217536153863713),
  'u_stat': np.float64(1253.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.17124430774567234),
  'u_stat': np.float64(1169.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.7701664592968517),
  'p_raw': np.float64(0.07701664592968517),
  'u_stat': np.float64(993.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.0365441667205914),
  'p_raw': np.float64(0.0036544166720591403),
  'u_stat': np.float64(1327.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(0.061347078018374177),
  'p_raw': np.float64(0.006134707801837417),
  'u_stat': np.float64(662.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': np.float64(0.7511624083182533),
  'p_raw': np.float64(0.07511624083182533),
  'u_stat': np.float64(962.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.0008722390112641334),
  'p_raw': np.float64(8.722390112641334e-05),
  'u_stat': np.float64(1450.0)}]

--- Metric: rougeL ---

=== Wilcoxon rank-sum tests for metric 'rougeL' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1231.5000, p_raw = 0.9012, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1352.0000, p_raw = 0.004314, p_bonf = 0.04314 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 1269.5000, p_raw = 0.8958, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1448.5000, p_raw = 9.179e-05, p_bonf = 0.0009179 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 1320.5000, p_raw = 0.009366, p_bonf = 0.09366 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1233.0000, p_raw = 0.9094, p_bonf = 1 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1401.0000, p_raw = 0.0004339, p_bonf = 0.004339 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 747.0000, p_raw = 0.04033, p_bonf = 0.4033 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 918.5000, p_raw = 0.176, p_bonf = 1 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1361.0000, p_raw = 0.001434, p_bonf = 0.01434 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.9012423462704183),
  'u_stat': np.float64(1231.5)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.043144271798215776),
  'p_raw': np.float64(0.004314427179821578),
  'u_stat': np.float64(1352.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.8957866025609975),
  'u_stat': np.float64(1269.5)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.0009178981936194748),
  'p_raw': np.float64(9.178981936194748e-05),
  'u_stat': np.float64(1448.5)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.09365528169431593),
  'p_raw': np.float64(0.009365528169431593),
  'u_stat': np.float64(1320.5)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.9094359154591822),
  'u_stat': np.float64(1233.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.004339499054821808),
  'p_raw': np.float64(0.00043394990548218076),
  'u_stat': np.float64(1401.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(0.40333202282079417),
  'p_raw': np.float64(0.040333202282079414),
  'u_stat': np.float64(747.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.17597816868401317),
  'u_stat': np.float64(918.5)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.01433657795396199),
  'p_raw': np.float64(0.001433657795396199),
  'u_stat': np.float64(1361.0)}]

--- Metric: meteor ---

=== Wilcoxon rank-sum tests for metric 'meteor' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1234.0000, p_raw = 0.9149, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 876.0000, p_raw = 0.3159, p_bonf = 1 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 1805.0000, p_raw = 0.000132, p_bonf = 0.00132 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1616.0000, p_raw = 1.182e-07, p_bonf = 1.182e-06 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 882.0000, p_raw = 0.34, p_bonf = 1 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1815.0000, p_raw = 9.96e-05, p_bonf = 0.000996 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1602.0000, p_raw = 2.213e-07, p_bonf = 2.213e-06 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 1568.0000, p_raw = 4.064e-06, p_bonf = 4.064e-05 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 1320.0000, p_raw = 1.222e-07, p_bonf = 1.222e-06 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1292.0000, p_raw = 0.008867, p_bonf = 0.08867 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.9149046417869761),
  'u_stat': np.float64(1234.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.3159489431942337),
  'u_stat': np.float64(876.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.0013203951241330296),
  'p_raw': np.float64(0.00013203951241330295),
  'u_stat': np.float64(1805.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(1.1818352383277014e-06),
  'p_raw': np.float64(1.1818352383277014e-07),
  'u_stat': np.float64(1616.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.34003424946790384),
  'u_stat': np.float64(882.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': np.float64(0.000996012089599748),
  'p_raw': np.float64(9.960120895997481e-05),
  'u_stat': np.float64(1815.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(2.2127126808339686e-06),
  'p_raw': np.float64(2.2127126808339687e-07),
  'u_stat': np.float64(1602.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(4.0638803615364736e-05),
  'p_raw': np.float64(4.063880361536474e-06),
  'u_stat': np.float64(1568.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': np.float64(1.2216080506448069e-06),
  'p_raw': np.float64(1.2216080506448069e-07),
  'u_stat': np.float64(1320.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(0.08867180983668317),
  'p_raw': np.float64(0.008867180983668317),
  'u_stat': np.float64(1292.0)}]

================================================================================
INDIVIDUAL MODEL COMPARISONS - WILCOXON SIGNED-RANK TESTS
================================================================================

--- Metric: accuracy ---

=== Wilcoxon signed-rank tests for metric 'accuracy' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 16.5000, p_raw = 0.2059, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 20.0000, p_raw = 0.7389, p_bonf = 1 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 19.5000, p_raw = 0.08326, p_bonf = 0.8326 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 87.5000, p_raw = 0.04123, p_bonf = 0.4123 (n_paired = 39)
llama3-8b vs med42-70b: W = 18.0000, p_raw = 0.1317, p_bonf = 1 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 25.5000, p_raw = 0.01242, p_bonf = 0.1242 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 57.0000, p_raw = 0.1573, p_bonf = 1 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 0.0000, p_raw = 0.08326, p_bonf = 0.8326 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 32.0000, p_raw = 0.0707, p_bonf = 0.707 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 48.0000, p_raw = 0.001762, p_bonf = 0.01762 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.2059032107320683),
  'w_stat': np.float64(16.5)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.7388826803635273),
  'w_stat': np.float64(20.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.8326451666355039),
  'p_raw': np.float64(0.08326451666355039),
  'w_stat': np.float64(19.5)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.4122683333716368),
  'p_raw': np.float64(0.04122683333716368),
  'w_stat': np.float64(87.5)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.13166801602281422),
  'w_stat': np.float64(18.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.12419330651552268),
  'p_raw': np.float64(0.012419330651552268),
  'w_stat': np.float64(25.5)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.15729920705028505),
  'w_stat': np.float64(57.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.8326451666355039),
  'p_raw': np.float64(0.08326451666355039),
  'w_stat': np.float64(0.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': np.float64(0.7070114486598299),
  'p_raw': np.float64(0.07070114486598299),
  'w_stat': np.float64(32.0)},
 {'alpha': 0.05,
  'metric': 'accuracy',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.017617019214338062),
  'p_raw': np.float64(0.0017617019214338063),
  'w_stat': np.float64(48.0)}]

--- Metric: bert_scores ---

=== Wilcoxon signed-rank tests for metric 'bert_scores' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 622.0000, p_raw = 0.8859, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 284.0000, p_raw = 0.09182, p_bonf = 0.9182 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 399.0000, p_raw = 0.02067, p_bonf = 0.2067 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 188.0000, p_raw = 0.004087, p_bonf = 0.04087 (n_paired = 39)
llama3-8b vs med42-70b: W = 238.0000, p_raw = 0.01999, p_bonf = 0.1999 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 348.0000, p_raw = 0.004606, p_bonf = 0.04606 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 171.0000, p_raw = 0.001725, p_bonf = 0.01725 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 141.0000, p_raw = 0.0001597, p_bonf = 0.001597 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 177.0000, p_raw = 0.1694, p_bonf = 1 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 127.0000, p_raw = 0.0001201, p_bonf = 0.001201 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.8858855663449852),
  'w_stat': np.float64(622.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.9182406164836721),
  'p_raw': np.float64(0.0918240616483672),
  'w_stat': np.float64(284.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.20665866669398625),
  'p_raw': np.float64(0.020665866669398625),
  'w_stat': np.float64(399.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.04087226428964641),
  'p_raw': np.float64(0.004087226428964641),
  'w_stat': np.float64(188.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.19987935313110938),
  'p_raw': np.float64(0.01998793531311094),
  'w_stat': np.float64(238.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.04606102327862516),
  'p_raw': np.float64(0.004606102327862516),
  'w_stat': np.float64(348.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.017247816940653138),
  'p_raw': np.float64(0.0017247816940653138),
  'w_stat': np.float64(171.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.001597403588675661),
  'p_raw': np.float64(0.0001597403588675661),
  'w_stat': np.float64(141.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.16942174453288317),
  'w_stat': np.float64(177.0)},
 {'alpha': 0.05,
  'metric': 'bert_scores',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.001200967926706653),
  'p_raw': np.float64(0.00012009679267066531),
  'w_stat': np.float64(127.0)}]

--- Metric: bart_scores ---

=== Wilcoxon signed-rank tests for metric 'bart_scores' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 373.0000, p_raw = 0.009964, p_bonf = 0.09964 (n_paired = 50)
med42-8b vs med42-70b: W = 125.0000, p_raw = 5.45e-05, p_bonf = 0.000545 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 266.0000, p_raw = 0.0002118, p_bonf = 0.002118 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 130.0000, p_raw = 0.0001473, p_bonf = 0.001473 (n_paired = 39)
llama3-8b vs med42-70b: W = 34.0000, p_raw = 6.776e-09, p_bonf = 6.776e-08 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 393.0000, p_raw = 0.01758, p_bonf = 0.1758 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 39.0000, p_raw = 2.759e-08, p_bonf = 2.759e-07 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 3.0000, p_raw = 9.095e-12, p_bonf = 9.095e-11 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 221.0000, p_raw = 0.6083, p_bonf = 1 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 40.0000, p_raw = 3.164e-08, p_bonf = 3.164e-07 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': np.float64(0.09964281765361349),
  'p_raw': np.float64(0.009964281765361349),
  'w_stat': np.float64(373.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.0005449729542306159),
  'p_raw': np.float64(5.449729542306159e-05),
  'w_stat': np.float64(125.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.0021178724761483636),
  'p_raw': np.float64(0.00021178724761483636),
  'w_stat': np.float64(266.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.0014731190822203644),
  'p_raw': np.float64(0.00014731190822203644),
  'w_stat': np.float64(130.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(6.775735528208315e-08),
  'p_raw': np.float64(6.775735528208315e-09),
  'w_stat': np.float64(34.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.17575449774248852),
  'p_raw': np.float64(0.017575449774248852),
  'w_stat': np.float64(393.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(2.759043127298355e-07),
  'p_raw': np.float64(2.759043127298355e-08),
  'w_stat': np.float64(39.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(9.094947017729282e-11),
  'p_raw': np.float64(9.094947017729282e-12),
  'w_stat': np.float64(3.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.608313562348485),
  'w_stat': np.float64(221.0)},
 {'alpha': 0.05,
  'metric': 'bart_scores',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(3.1635863706469536e-07),
  'p_raw': np.float64(3.1635863706469536e-08),
  'w_stat': np.float64(40.0)}]

--- Metric: alignscore ---

=== Wilcoxon signed-rank tests for metric 'alignscore' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 520.0000, p_raw = 0.2612, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 400.0000, p_raw = 0.8995, p_bonf = 1 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 215.0000, p_raw = 1.876e-05, p_bonf = 0.0001876 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 248.0000, p_raw = 0.0475, p_bonf = 0.475 (n_paired = 39)
llama3-8b vs med42-70b: W = 247.0000, p_raw = 0.02784, p_bonf = 0.2784 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 378.0000, p_raw = 0.01153, p_bonf = 0.1153 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 227.0000, p_raw = 0.02216, p_bonf = 0.2216 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 174.0000, p_raw = 0.001107, p_bonf = 0.01107 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 187.0000, p_raw = 0.2395, p_bonf = 1 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 112.0000, p_raw = 4.081e-05, p_bonf = 0.0004081 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.26120880915335754),
  'w_stat': np.float64(520.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.8995151304461615),
  'w_stat': np.float64(400.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.00018759072675678112),
  'p_raw': np.float64(1.8759072675678112e-05),
  'w_stat': np.float64(215.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.47504779151495313),
  'p_raw': np.float64(0.04750477915149531),
  'w_stat': np.float64(248.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.27836266126541886),
  'p_raw': np.float64(0.027836266126541886),
  'w_stat': np.float64(247.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.11529687649083797),
  'p_raw': np.float64(0.011529687649083797),
  'w_stat': np.float64(378.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.22157881470775465),
  'p_raw': np.float64(0.022157881470775465),
  'w_stat': np.float64(227.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.011065220933232922),
  'p_raw': np.float64(0.0011065220933232922),
  'w_stat': np.float64(174.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.23947724793106318),
  'w_stat': np.float64(187.0)},
 {'alpha': 0.05,
  'metric': 'alignscore',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.0004081365477759391),
  'p_raw': np.float64(4.081365477759391e-05),
  'w_stat': np.float64(112.0)}]

--- Metric: rougeL ---

=== Wilcoxon signed-rank tests for metric 'rougeL' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 570.0000, p_raw = 0.521, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 198.0000, p_raw = 0.00369, p_bonf = 0.0369 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 561.0000, p_raw = 0.4665, p_bonf = 1 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 113.0000, p_raw = 4.4e-05, p_bonf = 0.00044 (n_paired = 39)
llama3-8b vs med42-70b: W = 164.0000, p_raw = 0.0006385, p_bonf = 0.006385 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 517.0000, p_raw = 0.2491, p_bonf = 1 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 157.0000, p_raw = 0.0007941, p_bonf = 0.007941 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 159.0000, p_raw = 0.001266, p_bonf = 0.01266 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 172.0000, p_raw = 0.1406, p_bonf = 1 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 141.0000, p_raw = 0.0003022, p_bonf = 0.003022 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.5210251168620434),
  'w_stat': np.float64(570.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.03690388592076488),
  'p_raw': np.float64(0.003690388592076488),
  'w_stat': np.float64(198.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.46647302992520423),
  'w_stat': np.float64(561.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.0004399772296892479),
  'p_raw': np.float64(4.399772296892479e-05),
  'w_stat': np.float64(113.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.006384575062838849),
  'p_raw': np.float64(0.0006384575062838849),
  'w_stat': np.float64(164.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.24908489906916564),
  'w_stat': np.float64(517.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.007940735413285438),
  'p_raw': np.float64(0.0007940735413285438),
  'w_stat': np.float64(157.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.012658809279067085),
  'p_raw': np.float64(0.0012658809279067085),
  'w_stat': np.float64(159.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.14060599450021982),
  'w_stat': np.float64(172.0)},
 {'alpha': 0.05,
  'metric': 'rougeL',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.0030219102700357325),
  'p_raw': np.float64(0.00030219102700357325),
  'w_stat': np.float64(141.0)}]

--- Metric: meteor ---

=== Wilcoxon signed-rank tests for metric 'meteor' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 635.0000, p_raw = 0.9847, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 245.0000, p_raw = 0.0259, p_bonf = 0.259 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 136.0000, p_raw = 1.508e-07, p_bonf = 1.508e-06 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 56.0000, p_raw = 2.321e-07, p_bonf = 2.321e-06 (n_paired = 39)
llama3-8b vs med42-70b: W = 299.0000, p_raw = 0.1387, p_bonf = 1 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 201.0000, p_raw = 8.873e-06, p_bonf = 8.873e-05 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 52.0000, p_raw = 1.454e-07, p_bonf = 1.454e-06 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 94.0000, p_raw = 4.933e-06, p_bonf = 4.933e-05 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 13.0000, p_raw = 8.196e-08, p_bonf = 8.196e-07 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 197.0000, p_raw = 0.006247, p_bonf = 0.06247 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.9847343420165711),
  'w_stat': np.float64(635.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.2589861064188881),
  'p_raw': np.float64(0.02589861064188881),
  'w_stat': np.float64(245.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(1.5079201176604329e-06),
  'p_raw': np.float64(1.507920117660433e-07),
  'w_stat': np.float64(136.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(2.3208485799841583e-06),
  'p_raw': np.float64(2.3208485799841583e-07),
  'w_stat': np.float64(56.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.13868038851069286),
  'w_stat': np.float64(299.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(8.872967917739061e-05),
  'p_raw': np.float64(8.87296791773906e-06),
  'w_stat': np.float64(201.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(1.4542820281349123e-06),
  'p_raw': np.float64(1.4542820281349123e-07),
  'w_stat': np.float64(52.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(4.9328082241117954e-05),
  'p_raw': np.float64(4.9328082241117954e-06),
  'w_stat': np.float64(94.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': np.float64(8.195638656616211e-07),
  'p_raw': np.float64(8.195638656616211e-08),
  'w_stat': np.float64(13.0)},
 {'alpha': 0.05,
  'metric': 'meteor',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(0.06246893579373136),
  'p_raw': np.float64(0.006246893579373136),
  'w_stat': np.float64(197.0)}]

================================================================================
INDIVIDUAL MODEL COMPARISONS - WEIGHTED NORMALIZED SCORE
================================================================================
--- Wilcoxon Rank-Sum Test ---

=== Wilcoxon rank-sum tests for metric 'weighted_normalized_score' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: U = 1128.0000, p_raw = 0.4023, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs med42-70b: U = 1224.0000, p_raw = 0.06955, p_bonf = 0.6955 (n1 = 50, n2 = 40)
med42-8b vs llama3.3-70b: U = 1046.0000, p_raw = 0.1606, p_bonf = 1 (n1 = 50, n2 = 50)
med42-8b vs deepseek-r1-1.5: U = 1557.0000, p_raw = 1.521e-06, p_bonf = 1.521e-05 (n1 = 50, n2 = 39)
llama3-8b vs med42-70b: U = 1318.0000, p_raw = 0.009935, p_bonf = 0.09935 (n1 = 50, n2 = 40)
llama3-8b vs llama3.3-70b: U = 1158.0000, p_raw = 0.5282, p_bonf = 1 (n1 = 50, n2 = 50)
llama3-8b vs deepseek-r1-1.5: U = 1597.0000, p_raw = 2.76e-07, p_bonf = 2.76e-06 (n1 = 50, n2 = 39)
med42-70b vs llama3.3-70b: U = 653.0000, p_raw = 0.004899, p_bonf = 0.04899 (n1 = 40, n2 = 50)
med42-70b vs deepseek-r1-1.5: U = 1102.0000, p_raw = 0.001618, p_bonf = 0.01618 (n1 = 40, n2 = 39)
llama3.3-70b vs deepseek-r1-1.5: U = 1607.0000, p_raw = 1.771e-07, p_bonf = 1.771e-06 (n1 = 50, n2 = 39)
[{'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.40225589012941765),
  'u_stat': np.float64(1128.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.6955228258855256),
  'p_raw': np.float64(0.06955228258855256),
  'u_stat': np.float64(1224.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.16064894403312135),
  'u_stat': np.float64(1046.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(1.5213305097769377e-05),
  'p_raw': np.float64(1.5213305097769378e-06),
  'u_stat': np.float64(1557.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n1': 50,
  'n2': 40,
  'p_bonf': np.float64(0.09934712127222049),
  'p_raw': np.float64(0.009934712127222049),
  'u_stat': np.float64(1318.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n1': 50,
  'n2': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.5281814272943972),
  'u_stat': np.float64(1158.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(2.7595098350117566e-06),
  'p_raw': np.float64(2.759509835011757e-07),
  'u_stat': np.float64(1597.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n1': 40,
  'n2': 50,
  'p_bonf': np.float64(0.048994279903653806),
  'p_raw': np.float64(0.0048994279903653805),
  'u_stat': np.float64(653.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 40,
  'n2': 39,
  'p_bonf': np.float64(0.016184174703628006),
  'p_raw': np.float64(0.0016184174703628004),
  'u_stat': np.float64(1102.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n1': 50,
  'n2': 39,
  'p_bonf': np.float64(1.7713264265222183e-06),
  'p_raw': np.float64(1.7713264265222183e-07),
  'u_stat': np.float64(1607.0)}]
--- Wilcoxon Signed-Rank Test ---

=== Wilcoxon signed-rank tests for metric 'weighted_normalized_score' using df_no_error_norm (2-tailed, Bonferroni corrected) ===
med42-8b vs llama3-8b: W = 540.0000, p_raw = 0.3522, p_bonf = 1 (n_paired = 50)
med42-8b vs med42-70b: W = 257.0000, p_raw = 0.03945, p_bonf = 0.3945 (n_paired = 40)
med42-8b vs llama3.3-70b: W = 430.0000, p_raw = 0.04503, p_bonf = 0.4503 (n_paired = 50)
med42-8b vs deepseek-r1-1.5: W = 76.0000, p_raw = 1.899e-06, p_bonf = 1.899e-05 (n_paired = 39)
llama3-8b vs med42-70b: W = 154.0000, p_raw = 0.0003572, p_bonf = 0.003572 (n_paired = 40)
llama3-8b vs llama3.3-70b: W = 466.0000, p_raw = 0.09909, p_bonf = 0.9909 (n_paired = 50)
llama3-8b vs deepseek-r1-1.5: W = 58.0000, p_raw = 2.913e-07, p_bonf = 2.913e-06 (n_paired = 39)
med42-70b vs llama3.3-70b: W = 118.0000, p_raw = 3.295e-05, p_bonf = 0.0003295 (n_paired = 40)
med42-70b vs deepseek-r1-1.5: W = 106.0000, p_raw = 0.004436, p_bonf = 0.04436 (n_paired = 31)
llama3.3-70b vs deepseek-r1-1.5: W = 41.0000, p_raw = 3.621e-08, p_bonf = 3.621e-07 (n_paired = 39)
[{'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'llama3-8b',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.35217499509373695),
  'w_stat': np.float64(540.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.3944668134499807),
  'p_raw': np.float64(0.03944668134499807),
  'w_stat': np.float64(257.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.45027510990479414),
  'p_raw': np.float64(0.045027510990479414),
  'w_stat': np.float64(430.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(1.898664777399972e-05),
  'p_raw': np.float64(1.898664777399972e-06),
  'w_stat': np.float64(76.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'med42-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.0035718994149647187),
  'p_raw': np.float64(0.0003571899414964719),
  'w_stat': np.float64(154.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'llama3.3-70b',
  'n_paired': 50,
  'p_bonf': np.float64(0.9908592454842413),
  'p_raw': np.float64(0.09908592454842413),
  'w_stat': np.float64(466.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3-8b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(2.9125658329576254e-06),
  'p_raw': np.float64(2.9125658329576254e-07),
  'w_stat': np.float64(58.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-70b',
  'model_2': 'llama3.3-70b',
  'n_paired': 40,
  'p_bonf': np.float64(0.0003295479837106541),
  'p_raw': np.float64(3.295479837106541e-05),
  'w_stat': np.float64(118.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'med42-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 31,
  'p_bonf': np.float64(0.044362908229231834),
  'p_raw': np.float64(0.0044362908229231834),
  'w_stat': np.float64(106.0)},
 {'alpha': 0.05,
  'metric': 'weighted_normalized_score',
  'model_1': 'llama3.3-70b',
  'model_2': 'deepseek-r1-1.5',
  'n_paired': 39,
  'p_bonf': np.float64(3.621244104579091e-07),
  'p_raw': np.float64(3.621244104579091e-08),
  'w_stat': np.float64(41.0)}]

================================================================================
CATEGORY-BASED ANALYSIS - WILCOXON RANK-SUM TESTS
================================================================================

--- Metric: accuracy ---

=== Wilcoxon rank-sum tests for metric 'accuracy' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 4400.0000, p_raw = 0.7429, p_bonf = 1 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2224.5000, p_raw = 0.004643, p_bonf = 0.01393 (n1 = 90, n2 = 39)
llama vs deepseek: U = 2515.0000, p_raw = 0.001657, p_bonf = 0.004971 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'accuracy',
  'n1': 90,
  'n2': 100,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.7428961147624418),
  'u_stat': np.float64(4400.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'accuracy',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(0.013930078759569291),
  'p_raw': np.float64(0.004643359586523097),
  'u_stat': np.float64(2224.5)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'accuracy',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(0.004971099870611127),
  'p_raw': np.float64(0.001657033290203709),
  'u_stat': np.float64(2515.0)}]

--- Metric: bert_scores ---

=== Wilcoxon rank-sum tests for metric 'bert_scores' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 4062.0000, p_raw = 0.2477, p_bonf = 0.7431 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2197.0000, p_raw = 0.02357, p_bonf = 0.0707 (n1 = 90, n2 = 39)
llama vs deepseek: U = 2583.0000, p_raw = 0.003025, p_bonf = 0.009074 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'bert_scores',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(0.7431319620448605),
  'p_raw': np.float64(0.2477106540149535),
  'u_stat': np.float64(4062.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'bert_scores',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(0.07070342196356925),
  'p_raw': np.float64(0.02356780732118975),
  'u_stat': np.float64(2197.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'bert_scores',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(0.009074417054543205),
  'p_raw': np.float64(0.003024805684847735),
  'u_stat': np.float64(2583.0)}]

--- Metric: bart_scores ---

=== Wilcoxon rank-sum tests for metric 'bart_scores' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 2774.0000, p_raw = 5.14e-06, p_bonf = 1.542e-05 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2372.0000, p_raw = 0.001569, p_bonf = 0.004708 (n1 = 90, n2 = 39)
llama vs deepseek: U = 3252.0000, p_raw = 1.051e-09, p_bonf = 3.152e-09 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'bart_scores',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(1.542082951541427e-05),
  'p_raw': np.float64(5.14027650513809e-06),
  'u_stat': np.float64(2774.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'bart_scores',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(0.004708142743829052),
  'p_raw': np.float64(0.001569380914609684),
  'u_stat': np.float64(2372.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'bart_scores',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(3.1518141919051553e-09),
  'p_raw': np.float64(1.0506047306350518e-09),
  'u_stat': np.float64(3252.0)}]

--- Metric: alignscore ---

=== Wilcoxon rank-sum tests for metric 'alignscore' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 3506.0000, p_raw = 0.008666, p_bonf = 0.026 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2215.0000, p_raw = 0.01845, p_bonf = 0.05536 (n1 = 90, n2 = 39)
llama vs deepseek: U = 2777.0000, p_raw = 0.0001068, p_bonf = 0.0003203 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'alignscore',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(0.025998728864438565),
  'p_raw': np.float64(0.008666242954812855),
  'u_stat': np.float64(3506.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'alignscore',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(0.05535757726965969),
  'p_raw': np.float64(0.01845252575655323),
  'u_stat': np.float64(2215.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'alignscore',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(0.00032027923646266426),
  'p_raw': np.float64(0.00010675974548755474),
  'u_stat': np.float64(2777.0)}]

--- Metric: rougeL ---

=== Wilcoxon rank-sum tests for metric 'rougeL' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 3927.5000, p_raw = 0.1307, p_bonf = 0.3921 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2367.0000, p_raw = 0.001713, p_bonf = 0.005139 (n1 = 90, n2 = 39)
llama vs deepseek: U = 2762.0000, p_raw = 0.0001421, p_bonf = 0.0004264 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'rougeL',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(0.39212770056383867),
  'p_raw': np.float64(0.13070923352127956),
  'u_stat': np.float64(3927.5)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'rougeL',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(0.005139028450284125),
  'p_raw': np.float64(0.0017130094834280417),
  'u_stat': np.float64(2367.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'rougeL',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(0.00042638566013815566),
  'p_raw': np.float64(0.00014212855337938522),
  'u_stat': np.float64(2762.0)}]

--- Metric: meteor ---

=== Wilcoxon rank-sum tests for metric 'meteor' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 5725.0000, p_raw = 0.001215, p_bonf = 0.003646 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2936.0000, p_raw = 1.414e-09, p_bonf = 4.243e-09 (n1 = 90, n2 = 39)
llama vs deepseek: U = 2894.0000, p_raw = 9.725e-06, p_bonf = 2.918e-05 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'meteor',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(0.003645742792092759),
  'p_raw': np.float64(0.001215247597364253),
  'u_stat': np.float64(5725.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'meteor',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(4.242830619032684e-09),
  'p_raw': np.float64(1.4142768730108947e-09),
  'u_stat': np.float64(2936.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'meteor',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(2.91754015033474e-05),
  'p_raw': np.float64(9.725133834449132e-06),
  'u_stat': np.float64(2894.0)}]

================================================================================
CATEGORY-BASED ANALYSIS - WILCOXON SIGNED-RANK TESTS
================================================================================

--- Metric: accuracy ---

=== Wilcoxon signed-rank tests for metric 'accuracy' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 36.0000, p_raw = 0.7963, p_bonf = 1 (n_paired = 50)
med42 vs deepseek: W = 83.5000, p_raw = 0.02328, p_bonf = 0.06985 (n_paired = 39)
llama vs deepseek: W = 89.5000, p_raw = 0.01372, p_bonf = 0.04117 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'accuracy',
  'n_paired': 50,
  'p_bonf': 1.0,
  'p_raw': np.float64(0.7962534147376392),
  'w_stat': np.float64(36.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'accuracy',
  'n_paired': 39,
  'p_bonf': np.float64(0.06985100438834577),
  'p_raw': np.float64(0.02328366812944859),
  'w_stat': np.float64(83.5)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'accuracy',
  'n_paired': 39,
  'p_bonf': np.float64(0.04117192620807514),
  'p_raw': np.float64(0.013723975402691713),
  'w_stat': np.float64(89.5)}]

--- Metric: bert_scores ---

=== Wilcoxon signed-rank tests for metric 'bert_scores' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 371.0000, p_raw = 0.009392, p_bonf = 0.02818 (n_paired = 50)
med42 vs deepseek: W = 202.0000, p_raw = 0.007835, p_bonf = 0.0235 (n_paired = 39)
llama vs deepseek: W = 134.0000, p_raw = 0.0001923, p_bonf = 0.000577 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'bert_scores',
  'n_paired': 50,
  'p_bonf': np.float64(0.02817634631461452),
  'p_raw': np.float64(0.00939211543820484),
  'w_stat': np.float64(371.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'bert_scores',
  'n_paired': 39,
  'p_bonf': np.float64(0.02350378287883359),
  'p_raw': np.float64(0.00783459429294453),
  'w_stat': np.float64(202.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'bert_scores',
  'n_paired': 39,
  'p_bonf': np.float64(0.0005770135976490565),
  'p_raw': np.float64(0.00019233786588301882),
  'w_stat': np.float64(134.0)}]

--- Metric: bart_scores ---

=== Wilcoxon signed-rank tests for metric 'bart_scores' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 102.0000, p_raw = 1.064e-08, p_bonf = 3.193e-08 (n_paired = 50)
med42 vs deepseek: W = 173.0000, p_raw = 0.001917, p_bonf = 0.005752 (n_paired = 39)
llama vs deepseek: W = 28.0000, p_raw = 5.395e-09, p_bonf = 1.619e-08 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'bart_scores',
  'n_paired': 50,
  'p_bonf': np.float64(3.1926301602425156e-08),
  'p_raw': np.float64(1.0642100534141719e-08),
  'w_stat': np.float64(102.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'bart_scores',
  'n_paired': 39,
  'p_bonf': np.float64(0.005752162185672205),
  'p_raw': np.float64(0.0019173873952240683),
  'w_stat': np.float64(173.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'bart_scores',
  'n_paired': 39,
  'p_bonf': np.float64(1.618536771275103e-08),
  'p_raw': np.float64(5.39512257091701e-09),
  'w_stat': np.float64(28.0)}]

--- Metric: alignscore ---

=== Wilcoxon signed-rank tests for metric 'alignscore' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 264.0000, p_raw = 0.0001942, p_bonf = 0.0005825 (n_paired = 50)
med42 vs deepseek: W = 241.0000, p_raw = 0.03723, p_bonf = 0.1117 (n_paired = 39)
llama vs deepseek: W = 128.0000, p_raw = 0.0001286, p_bonf = 0.0003858 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'alignscore',
  'n_paired': 50,
  'p_bonf': np.float64(0.0005824734442185786),
  'p_raw': np.float64(0.0001941578147395262),
  'w_stat': np.float64(264.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'alignscore',
  'n_paired': 39,
  'p_bonf': np.float64(0.1117036551295314),
  'p_raw': np.float64(0.0372345517098438),
  'w_stat': np.float64(241.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'alignscore',
  'n_paired': 39,
  'p_bonf': np.float64(0.0003858329182548914),
  'p_raw': np.float64(0.00012861097275163047),
  'w_stat': np.float64(128.0)}]

--- Metric: rougeL ---

=== Wilcoxon signed-rank tests for metric 'rougeL' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 440.0000, p_raw = 0.05674, p_bonf = 0.1702 (n_paired = 50)
med42 vs deepseek: W = 148.0000, p_raw = 0.0004663, p_bonf = 0.001399 (n_paired = 39)
llama vs deepseek: W = 120.0000, p_raw = 7.348e-05, p_bonf = 0.0002204 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'rougeL',
  'n_paired': 50,
  'p_bonf': np.float64(0.17021609596899623),
  'p_raw': np.float64(0.056738698656332076),
  'w_stat': np.float64(440.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'rougeL',
  'n_paired': 39,
  'p_bonf': np.float64(0.0013987803176860325),
  'p_raw': np.float64(0.00046626010589534417),
  'w_stat': np.float64(148.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'rougeL',
  'n_paired': 39,
  'p_bonf': np.float64(0.00022044419529265724),
  'p_raw': np.float64(7.348139843088575e-05),
  'w_stat': np.float64(120.0)}]

--- Metric: meteor ---

=== Wilcoxon signed-rank tests for metric 'meteor' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 139.0000, p_raw = 1.867e-07, p_bonf = 5.602e-07 (n_paired = 50)
med42 vs deepseek: W = 19.0000, p_raw = 1.117e-09, p_bonf = 3.351e-09 (n_paired = 39)
llama vs deepseek: W = 79.0000, p_raw = 2.529e-06, p_bonf = 7.587e-06 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'meteor',
  'n_paired': 50,
  'p_bonf': np.float64(5.601978081415382e-07),
  'p_raw': np.float64(1.8673260271384606e-07),
  'w_stat': np.float64(139.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'meteor',
  'n_paired': 39,
  'p_bonf': np.float64(3.3505784813314676e-09),
  'p_raw': np.float64(1.1168594937771559e-09),
  'w_stat': np.float64(19.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'meteor',
  'n_paired': 39,
  'p_bonf': np.float64(7.587346772197634e-06),
  'p_raw': np.float64(2.5291155907325447e-06),
  'w_stat': np.float64(79.0)}]

================================================================================
CATEGORY-BASED ANALYSIS - WEIGHTED NORMALIZED SCORE
================================================================================
--- Wilcoxon Rank-Sum Test ---

=== Wilcoxon rank-sum tests for metric 'weighted_normalized_score' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: U = 3509.0000, p_raw = 0.00887, p_bonf = 0.02661 (n1 = 90, n2 = 100)
med42 vs deepseek: U = 2659.0000, p_raw = 3.598e-06, p_bonf = 1.079e-05 (n1 = 90, n2 = 39)
llama vs deepseek: U = 3204.0000, p_raw = 4.19e-09, p_bonf = 1.257e-08 (n1 = 100, n2 = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'weighted_normalized_score',
  'n1': 90,
  'n2': 100,
  'p_bonf': np.float64(0.02661028544056568),
  'p_raw': np.float64(0.008870095146855226),
  'u_stat': np.float64(3509.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'weighted_normalized_score',
  'n1': 90,
  'n2': 39,
  'p_bonf': np.float64(1.0794733865234124e-05),
  'p_raw': np.float64(3.598244621744708e-06),
  'u_stat': np.float64(2659.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'weighted_normalized_score',
  'n1': 100,
  'n2': 39,
  'p_bonf': np.float64(1.2571045455949372e-08),
  'p_raw': np.float64(4.190348485316457e-09),
  'u_stat': np.float64(3204.0)}]
--- Wilcoxon Signed-Rank Test ---

=== Wilcoxon signed-rank tests for metric 'weighted_normalized_score' by CATEGORY using df_no_error_norm (2-tailed, Bonferroni corrected) ===
Categories: ['med42', 'llama', 'deepseek']
Category groupings: {'med42': ['med42-8b', 'med42-70b'], 'llama': ['llama3-8b', 'llama3.3-70b'], 'deepseek': ['deepseek-r1-1.5']}
med42 vs llama: W = 306.0000, p_raw = 0.001064, p_bonf = 0.003193 (n_paired = 50)
med42 vs deepseek: W = 81.0000, p_raw = 3.051e-06, p_bonf = 9.153e-06 (n_paired = 39)
llama vs deepseek: W = 35.0000, p_raw = 1.568e-08, p_bonf = 4.704e-08 (n_paired = 39)
[{'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'llama',
  'metric': 'weighted_normalized_score',
  'n_paired': 50,
  'p_bonf': np.float64(0.0031932891143551245),
  'p_raw': np.float64(0.0010644297047850415),
  'w_stat': np.float64(306.0)},
 {'alpha': 0.05,
  'category_1': 'med42',
  'category_2': 'deepseek',
  'metric': 'weighted_normalized_score',
  'n_paired': 39,
  'p_bonf': np.float64(9.15259079192765e-06),
  'p_raw': np.float64(3.050863597309217e-06),
  'w_stat': np.float64(81.0)},
 {'alpha': 0.05,
  'category_1': 'llama',
  'category_2': 'deepseek',
  'metric': 'weighted_normalized_score',
  'n_paired': 39,
  'p_bonf': np.float64(4.703906597569585e-08),
  'p_raw': np.float64(1.5679688658565283e-08),
  'w_stat': np.float64(35.0)}]
